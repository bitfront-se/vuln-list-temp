{
  "configurations": {
    "CVE_data_version": "4.0",
    "nodes": []
  },
  "cve": {
    "CVE_data_meta": {
      "ASSIGNER": "security-advisories@github.com",
      "ID": "CVE-2025-47277"
    },
    "data_format": "MITRE",
    "data_type": "CVE",
    "data_version": "4.0",
    "description": {
      "description_data": [
        {
          "lang": "en",
          "value": "vLLM, an inference and serving engine for large language models (LLMs), has an issue in versions 0.6.5 through 0.8.4 that ONLY impacts environments using the `PyNcclPipe` KV cache transfer integration with the V0 engine. No other configurations are affected. vLLM supports the use of the `PyNcclPipe` class to establish a peer-to-peer communication domain for data transmission between distributed nodes. The GPU-side KV-Cache transmission is implemented through the `PyNcclCommunicator` class, while CPU-side control message passing is handled via the `send_obj` and `recv_obj` methods on the CPU side.? The intention was that this interface should only be exposed to a private network using the IP address specified by the `--kv-ip` CLI parameter. The vLLM documentation covers how this must be limited to a secured network. The default and intentional behavior from PyTorch is that the `TCPStore` interface listens on ALL interfaces, regardless of what IP address is provided. The IP address given was only used as a client-side address to use. vLLM was fixed to use a workaround to force the `TCPStore` instance to bind its socket to a specified private interface. As of version 0.8.5, vLLM limits the `TCPStore` socket to the private interface as configured."
        }
      ]
    },
    "problemtype": {
      "problemtype_data": [
        {
          "description": [
            {
              "lang": "en",
              "value": "CWE-502"
            }
          ]
        }
      ]
    },
    "references": {
      "reference_data": [
        {
          "name": "https://docs.vllm.ai/en/latest/deployment/security.html",
          "refsource": "",
          "tags": [],
          "url": "https://docs.vllm.ai/en/latest/deployment/security.html"
        },
        {
          "name": "https://github.com/vllm-project/vllm/commit/0d6e187e88874c39cda7409cf673f9e6546893e7",
          "refsource": "",
          "tags": [],
          "url": "https://github.com/vllm-project/vllm/commit/0d6e187e88874c39cda7409cf673f9e6546893e7"
        },
        {
          "name": "https://github.com/vllm-project/vllm/pull/15988",
          "refsource": "",
          "tags": [],
          "url": "https://github.com/vllm-project/vllm/pull/15988"
        },
        {
          "name": "https://github.com/vllm-project/vllm/security/advisories/GHSA-hjq4-87xh-g4fv",
          "refsource": "",
          "tags": [],
          "url": "https://github.com/vllm-project/vllm/security/advisories/GHSA-hjq4-87xh-g4fv"
        }
      ]
    }
  },
  "impact": {},
  "lastModifiedDate": "2025-05-20T18:15Z",
  "publishedDate": "2025-05-20T18:15Z"
}